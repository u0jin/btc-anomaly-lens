import numpy as np
import pandas as pd
from collections import Counter, defaultdict
from datetime import datetime, timedelta
import math
import re
from typing import Dict, List, Tuple, Optional
import streamlit as st

class ExchangePatternAnalyzer:
    """ê±°ë˜ì†Œ ì£¼ì†Œ íŒ¨í„´ ë¶„ì„ê¸° - ì—”íŠ¸ë¡œí”¼, ê·œì¹™ì„±, íŠ¹ì§• ì¶”ì¶œ"""
    
    def __init__(self):
        self.exchange_patterns = {
            'Binance': {
                'entropy_range': (3.2, 3.8),
                'amount_patterns': ['round_numbers', 'high_volume'],
                'time_patterns': ['regular_intervals', 'batch_processing'],
                'address_patterns': ['bc1_prefix', 'multiple_outputs']
            },
            'Upbit': {
                'entropy_range': (3.0, 3.6),
                'amount_patterns': ['korean_market', 'krw_conversion'],
                'time_patterns': ['korean_timezone', 'business_hours'],
                'address_patterns': ['legacy_format', 'single_output']
            },
            'Coinbase': {
                'entropy_range': (3.4, 3.9),
                'amount_patterns': ['usd_conversion', 'institutional'],
                'time_patterns': ['us_timezone', 'regular_schedules'],
                'address_patterns': ['segwit_format', 'multiple_outputs']
            },
            'OKX': {
                'entropy_range': (3.1, 3.7),
                'amount_patterns': ['asian_market', 'high_frequency'],
                'time_patterns': ['asian_timezone', 'continuous_trading'],
                'address_patterns': ['mixed_formats', 'high_volume']
            }
        }
    
    def calculate_address_entropy(self, address: str) -> float:
        """ì£¼ì†Œì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì •ë³´ ì´ë¡  ê¸°ë°˜)"""
        if not address:
            return 0.0
        
        # ë¬¸ì ë¹ˆë„ ê³„ì‚°
        char_freq = Counter(address)
        total_chars = len(address)
        
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: H = -Î£(p_i * log2(p_i))
        entropy = 0.0
        for count in char_freq.values():
            probability = count / total_chars
            if probability > 0:
                entropy -= probability * math.log2(probability)
        
        return entropy
    
    def analyze_amount_patterns(self, tx_list: List[Dict]) -> Dict:
        """ê¸ˆì•¡ íŒ¨í„´ ë¶„ì„"""
        amounts = [tx.get('amount', 0) for tx in tx_list if tx.get('amount', 0) > 0]
        if not amounts:
            return {}
        
        analysis = {
            'total_volume': sum(amounts),
            'avg_amount': np.mean(amounts),
            'std_amount': np.std(amounts),
            'min_amount': min(amounts),
            'max_amount': max(amounts),
            'round_numbers': 0,
            'high_volume': 0,
            'krw_conversion': 0,
            'usd_conversion': 0
        }
        
        # ë°˜ì˜¬ë¦¼ ìˆ«ì íŒ¨í„´ (1000, 10000, 100000 ë“±)
        for amount in amounts:
            if amount % 1000 == 0:
                analysis['round_numbers'] += 1
            if amount > 1000000:  # 1 BTC ì´ìƒ
                analysis['high_volume'] += 1
            # KRW í™˜ì‚° íŒ¨í„´ (ì•½ 50,000,000ì› = 1 BTC ê¸°ì¤€)
            if 45000000 <= amount <= 55000000:
                analysis['krw_conversion'] += 1
            # USD í™˜ì‚° íŒ¨í„´ (ì•½ 40,000 USD = 1 BTC ê¸°ì¤€)
            if 35000000 <= amount <= 45000000:
                analysis['usd_conversion'] += 1
        
        return analysis
    
    def analyze_time_patterns(self, tx_list: List[Dict]) -> Dict:
        """ì‹œê°„ íŒ¨í„´ ë¶„ì„"""
        timestamps = []
        for tx in tx_list:
            try:
                if 'timestamp' in tx:
                    ts = datetime.fromisoformat(tx['timestamp'])
                    timestamps.append(ts)
            except:
                continue
        
        if len(timestamps) < 2:
            return {}
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        hours = [ts.hour for ts in timestamps]
        hour_distribution = Counter(hours)
        
        # í•œêµ­ ì‹œê°„ëŒ€ (9-18ì‹œ) vs ë¯¸êµ­ ì‹œê°„ëŒ€ (14-23ì‹œ UTC)
        korean_hours = sum(1 for h in hours if 9 <= h <= 18)
        us_hours = sum(1 for h in hours if 14 <= h <= 23)
        
        # ê°„ê²© ë¶„ì„
        sorted_times = sorted(timestamps)
        intervals = [(t2 - t1).total_seconds() for t1, t2 in zip(sorted_times[:-1], sorted_times[1:])]
        
        analysis = {
            'total_transactions': len(timestamps),
            'korean_timezone_ratio': korean_hours / len(hours) if hours else 0,
            'us_timezone_ratio': us_hours / len(hours) if hours else 0,
            'avg_interval': np.mean(intervals) if intervals else 0,
            'std_interval': np.std(intervals) if intervals else 0,
            'regular_intervals': 0,
            'batch_processing': 0
        }
        
        # ì •ê·œ ê°„ê²© íŒ¨í„´ (30ì´ˆ-5ë¶„ ê°„ê²©)
        regular_count = sum(1 for interval in intervals if 30 <= interval <= 300)
        analysis['regular_intervals'] = regular_count / len(intervals) if intervals else 0
        
        # ë°°ì¹˜ ì²˜ë¦¬ íŒ¨í„´ (ì—°ì† íŠ¸ëœì­ì…˜)
        batch_count = sum(1 for interval in intervals if interval < 60)
        analysis['batch_processing'] = batch_count / len(intervals) if intervals else 0
        
        return analysis
    
    def analyze_address_patterns(self, tx_list: List[Dict]) -> Dict:
        """ì£¼ì†Œ íŒ¨í„´ ë¶„ì„"""
        addresses = []
        for tx in tx_list:
            if tx.get('to'):
                addresses.append(tx['to'])
            if tx.get('from'):
                addresses.append(tx['from'])
        
        if not addresses:
            return {}
        
        analysis = {
            'total_addresses': len(addresses),
            'unique_addresses': len(set(addresses)),
            'bc1_prefix': 0,
            'legacy_format': 0,
            'multiple_outputs': 0,
            'single_output': 0
        }
        
        for addr in addresses:
            if addr.startswith('bc1'):
                analysis['bc1_prefix'] += 1
            elif addr.startswith('1') or addr.startswith('3'):
                analysis['legacy_format'] += 1
        
        # ì¶œë ¥ ê°œìˆ˜ íŒ¨í„´
        for tx in tx_list:
            outputs = tx.get('outputs', [])
            if len(outputs) > 1:
                analysis['multiple_outputs'] += 1
            else:
                analysis['single_output'] += 1
        
        return analysis
    
    def calculate_exchange_similarity(self, tx_list: List[Dict], exchange_name: str) -> float:
        """íŠ¹ì • ê±°ë˜ì†Œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not tx_list:
            return 0.0
        
        # ê° íŒ¨í„´ ë¶„ì„
        amount_patterns = self.analyze_amount_patterns(tx_list)
        time_patterns = self.analyze_time_patterns(tx_list)
        address_patterns = self.analyze_address_patterns(tx_list)
        
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        addresses = [tx.get('to', '') for tx in tx_list if tx.get('to')]
        if addresses:
            avg_entropy = np.mean([self.calculate_address_entropy(addr) for addr in addresses])
        else:
            avg_entropy = 0.0
        
        # ê±°ë˜ì†Œë³„ íŒ¨í„´ ë§¤ì¹­
        exchange_pattern = self.exchange_patterns.get(exchange_name, {})
        similarity_score = 0.0
        total_checks = 0
        
        # ì—”íŠ¸ë¡œí”¼ ë§¤ì¹­
        if 'entropy_range' in exchange_pattern:
            min_entropy, max_entropy = exchange_pattern['entropy_range']
            if min_entropy <= avg_entropy <= max_entropy:
                similarity_score += 0.3
            total_checks += 1
        
        # ê¸ˆì•¡ íŒ¨í„´ ë§¤ì¹­
        for pattern in exchange_pattern.get('amount_patterns', []):
            if pattern in amount_patterns:
                if amount_patterns[pattern] > 0:
                    similarity_score += 0.2
            total_checks += 1
        
        # ì‹œê°„ íŒ¨í„´ ë§¤ì¹­
        for pattern in exchange_pattern.get('time_patterns', []):
            if pattern in time_patterns:
                if time_patterns[pattern] > 0.3:  # 30% ì´ìƒ
                    similarity_score += 0.2
            total_checks += 1
        
        # ì£¼ì†Œ íŒ¨í„´ ë§¤ì¹­
        for pattern in exchange_pattern.get('address_patterns', []):
            if pattern in address_patterns:
                if address_patterns[pattern] > 0:
                    similarity_score += 0.1
            total_checks += 1
        
        return (similarity_score / total_checks * 100) if total_checks > 0 else 0.0
    
    def identify_exchange_type(self, tx_list: List[Dict]) -> Dict:
        """ê±°ë˜ì†Œ ìœ í˜• ì‹ë³„"""
        if not tx_list:
            return {}
        
        results = {}
        
        # ì£¼ìš” ê±°ë˜ì†Œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        for exchange_name in self.exchange_patterns.keys():
            similarity = self.calculate_exchange_similarity(tx_list, exchange_name)
            results[exchange_name] = {
                'similarity': similarity,
                'confidence': 'high' if similarity > 70 else 'medium' if similarity > 50 else 'low'
            }
        
        # ê°€ì¥ ìœ ì‚¬í•œ ê±°ë˜ì†Œ ì°¾ê¸°
        best_match = max(results.items(), key=lambda x: x[1]['similarity'])
        
        return {
            'best_match': {
                'exchange': best_match[0],
                'similarity': best_match[1]['similarity'],
                'confidence': best_match[1]['confidence']
            },
            'all_matches': results,
            'analysis': {
                'amount_patterns': self.analyze_amount_patterns(tx_list),
                'time_patterns': self.analyze_time_patterns(tx_list),
                'address_patterns': self.analyze_address_patterns(tx_list),
                'entropy': self.calculate_address_entropy(tx_list[0].get('to', '')) if tx_list else 0.0
            }
        }
    
    def generate_exchange_report(self, tx_list: List[Dict]) -> str:
        """ê±°ë˜ì†Œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not tx_list:
            return "ë¶„ì„í•  íŠ¸ëœì­ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."
        
        identification = self.identify_exchange_type(tx_list)
        best_match = identification['best_match']
        analysis = identification['analysis']
        
        report = f"""
## ğŸ¦ ê±°ë˜ì†Œ íŒ¨í„´ ë¶„ì„ ë¦¬í¬íŠ¸

### ğŸ“Š ì£¼ìš” ë°œê²¬ì‚¬í•­
- **ê°€ì¥ ìœ ì‚¬í•œ ê±°ë˜ì†Œ**: {best_match['exchange']}
- **ìœ ì‚¬ë„**: {best_match['similarity']:.1f}%
- **ì‹ ë¢°ë„**: {best_match['confidence']}

### ğŸ” íŒ¨í„´ ë¶„ì„
**ê¸ˆì•¡ íŒ¨í„´:**
- ì´ ê±°ë˜ëŸ‰: {analysis['amount_patterns'].get('total_volume', 0):,.0f} satoshi
- í‰ê·  ê±°ë˜ëŸ‰: {analysis['amount_patterns'].get('avg_amount', 0):,.0f} satoshi
- ë°˜ì˜¬ë¦¼ ìˆ«ì íŒ¨í„´: {analysis['amount_patterns'].get('round_numbers', 0)}ê±´
- ëŒ€ìš©ëŸ‰ ê±°ë˜: {analysis['amount_patterns'].get('high_volume', 0)}ê±´

**ì‹œê°„ íŒ¨í„´:**
- í•œêµ­ ì‹œê°„ëŒ€ ë¹„ìœ¨: {analysis['time_patterns'].get('korean_timezone_ratio', 0):.1%}
- ë¯¸êµ­ ì‹œê°„ëŒ€ ë¹„ìœ¨: {analysis['time_patterns'].get('us_timezone_ratio', 0):.1%}
- ì •ê·œ ê°„ê²© íŒ¨í„´: {analysis['time_patterns'].get('regular_intervals', 0):.1%}
- ë°°ì¹˜ ì²˜ë¦¬ íŒ¨í„´: {analysis['time_patterns'].get('batch_processing', 0):.1%}

**ì£¼ì†Œ íŒ¨í„´:**
- bc1 ì ‘ë‘ì‚¬: {analysis['address_patterns'].get('bc1_prefix', 0)}ê±´
- ë ˆê±°ì‹œ í˜•ì‹: {analysis['address_patterns'].get('legacy_format', 0)}ê±´
- ë‹¤ì¤‘ ì¶œë ¥: {analysis['address_patterns'].get('multiple_outputs', 0)}ê±´

**ì—”íŠ¸ë¡œí”¼ ë¶„ì„:**
- ì£¼ì†Œ ì—”íŠ¸ë¡œí”¼: {analysis['entropy']:.2f} bits
"""
        
        return report

def analyze_exchange_patterns(tx_list: List[Dict]) -> Dict:
    """ê±°ë˜ì†Œ íŒ¨í„´ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜"""
    analyzer = ExchangePatternAnalyzer()
    return analyzer.identify_exchange_type(tx_list) 